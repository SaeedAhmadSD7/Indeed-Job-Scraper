{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEED JOB SCRAPPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start off by loading the libraries we will require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can scrap individual links from job posts and use Beautiful Soup to extract the details and job description.\n",
    "But it will be difficult to extract data in such a way because we will be redirected to a unique page each time on the company's website.\n",
    "Instead we will use selenium and click on individual post. After clicking on a job post, javascript runs and we are redirected to indeed's version of job post instead of that on company's page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets define a function which will scrap individual job posts from indeed.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_indeed(search,max_search = 100,save_csv = True):\n",
    "    # search_term is the keyword/designation to be searched\n",
    "    search_term = search.replace(' ','+')                                   \n",
    "    url = 'https://www.indeed.com/jobs?q={}&limit=50&radius=25&start=0'.format(search_term) \n",
    "    \n",
    "    # Start the browser and load the above URL\n",
    "    browser = webdriver.Chrome('C:/Program Files (x86)/chromedriver.exe')\n",
    "    browser.get(url)\n",
    "    \n",
    "    # A popup will appear when we finish loading the page. We cannot proceed without closing it.\n",
    "    # Hence send the command to the browser to close the popup\n",
    "    try:\n",
    "        close_button = browser.find_element_by_id('prime-popover-close-button')\n",
    "        close_button.click()\n",
    "    except (ElementNotVisibleException, NoSuchElementException):\n",
    "        print('No Popup')\n",
    "    \n",
    "    \n",
    "    # Rating is specified as width in the HTML code. According to this width, stars are assigned in the reviews section.\n",
    "    max_rating = browser.find_element_by_class_name('ratings').size['width']\n",
    "    \n",
    "    # Empty dataframe in which we will store our data scraped from job posts\n",
    "    data = pd.DataFrame(columns = ['Job_Title','Company','Estimated_Salary','Date_Posted','Link_Job_Company',\n",
    "                                   'Link_Job_Indeed','Location','Rating_In_Stars','Job_Description'])\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    \n",
    "    # Loop through the pages\n",
    "    for j in range(max_search // 50):\n",
    "        \n",
    "        # All the job posts have class 'row result clickcard' except the last one\n",
    "        # The last job post has class name 'lastRow row result clickcard'\n",
    "        # We will append it to our job_elements list\n",
    "        job_elements =  browser.find_elements_by_xpath(\"//div[@class='row result clickcard']\")\n",
    "        \n",
    "        try:\n",
    "            job_elements.append(browser.find_element_by_xpath(\"//div[@class='lastRow row result clickcard']\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Loop through the individual job posts\n",
    "        for i in range(len(job_elements)):\n",
    "            \n",
    "            \n",
    "            # The link to job post on company's website\n",
    "            redirect_to_comp_site = job_elements[i].find_element_by_tag_name('a').get_attribute('href')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Find estimated salary if mentioned, else assign NA\n",
    "            \n",
    "            try:\n",
    "                estimated_salary = job_elements[i].find_element_by_class_name('no-wrap').text\n",
    "            except NoSuchElementException:\n",
    "                estimated_salary = 'NA'\n",
    "            \n",
    "            \n",
    "            # Get the date when job was posted. Usually in N days ago.  \n",
    "            date = job_elements[i].find_element_by_class_name('date').text \n",
    "            \n",
    "            # Click on the job post\n",
    "            job_elements[i].click()\n",
    "\n",
    "            # Switch to the next browser tab \n",
    "            browser.switch_to_window(browser.window_handles[1])\n",
    "            \n",
    "            # Sleep for minimum 3 seconds because we dont want to create unnecessary load on Indeed's servers\n",
    "            sleep(3 + random.randint(0,3))\n",
    "            \n",
    "            # Link to the Indeed's version of job post\n",
    "            indeed_link = browser.current_url\n",
    "            \n",
    "            # Sometimes Selenium might start scraping before the page finishes loading or \n",
    "            # we might encounter '404 : Job not found error'\n",
    "            # Although these occurences are very rare we don't want our job scrapper to crash.\n",
    "            # Therefore we will retry before moving on.\n",
    "            # If the data was successfully scrapped then it will break out of the while loop\n",
    "            # If we encounter error it will retry again provided the retry count is below 5\n",
    "            \n",
    "            retry_count = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    retry_count += 1\n",
    "                    title =  browser.find_element_by_class_name('jobtitle').text\n",
    "                    post = browser.find_element_by_id('job_summary').text\n",
    "                    company = browser.find_element_by_class_name('company').text\n",
    "                    location = browser.find_element_by_class_name('location').text\n",
    "                    break\n",
    "                except NoSuchElementException:\n",
    "                    if retry_count > 5:\n",
    "                        print('Unable to fetch data. Skipping Page.....')\n",
    "                        break\n",
    "                    else:\n",
    "                        print('Unable to fetch data. Retrying....')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # We can fetch the job rating by class 'rating' but it rounds up the number stars. \n",
    "            # Hence it will be slightly less accurate\n",
    "            # Instead we will get the CSS attribute of 'rating' and convert it to number of stars ourselves.\n",
    "            try:\n",
    "                rtng_style_str = browser.find_element_by_class_name('rating').get_attribute('style')\n",
    "                rtng_style_int = float(rtng_style_str.split(' ')[1][:-3])\n",
    "                rating = (rtng_style_int*5)/max_rating\n",
    "            except NoSuchElementException:\n",
    "                rating = 'NA'\n",
    "\n",
    "            # For debugging purposes lets log the job post scrapped\n",
    "            print('Completed Post {} of Page {} - {}'.format(i+1,j+1,title))\n",
    "            \n",
    "            # Insert the data into our dataframe\n",
    "            data = data.append({'Job_Title':title,'Company':company,'Link_Job_Company':redirect_to_comp_site,'Estimated_Salary':estimated_salary,'Date_Posted':date,'Link_Job_Indeed':indeed_link,'Location':location,'Rating_In_Stars':rating,'Job_Description':post},ignore_index=True)    \n",
    "            # Close the window and go to the main page\n",
    "            browser.close()\n",
    "            browser.switch_to_window(browser.window_handles[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Change the URL, so as to move on to the next page\n",
    "\n",
    "        y += 50\n",
    "\n",
    "        url = url.replace('start=' + str(x),'start=' +str(x+50))\n",
    "        x+= 50\n",
    "\n",
    "        browser.get(url)\n",
    "        print('Moving on to page ' + str(j+2))\n",
    "        sleep(2)\n",
    "        \n",
    "        # A popover appears when we go to the next page. We will tell the browser to click on close button.\n",
    "        # Although so far for me it has appeared only on 2nd page but I have included the check for every page to be on safer side\n",
    "        try:\n",
    "            \n",
    "            browser.find_element_by_id('popover-x-button').click()\n",
    "        except:\n",
    "            print('No Newsletter Popup Found')\n",
    "            \n",
    "    # Save dataframe to csv     \n",
    "    if save_csv:\n",
    "        data.to_csv(search + '.csv')\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scrapped 1500 job posts for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrap_indeed('machine learning',1500) #Scrapped on 27th Jan\n",
    "scrap_indeed('artificial intelligence',1500) #Scrapped on 27th Jan\n",
    "scrap_indeed('data scientist',1500) #Scrapped on 28th Jan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv('Data Scientist.csv')\n",
    "ml = pd.read_csv('Machine Learning.csv')\n",
    "ai = pd.read_csv('Artificial Intelligence.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_main = pd.concat([ds,ml,ai],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicated_rows_bool = ds_main.duplicated(['Job_Title','Company','Job_Description'],keep = 'first')\n",
    "duplicated_rows_bool.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_main.drop_duplicates(subset=['Job_Title','Company','Job_Description'],inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by removing the stop words from Job Description column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "RE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "# replace '|'-->' ' and drop all stopwords\n",
    "\n",
    "#ds['Job_Description'] = ds['Job_Description'].str.lower()\n",
    "#/ to \" \" cuz or\n",
    "ds['Job_Description'] = ds['Job_Description'].replace([r'\\|', '/','•',RE_stopwords,'  ','\\n'], [' ',' ','', ' ',' ',' NextLineHere '], regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages_list = ['C\\\\+\\\\+','[^\\w]R[^\\w]', 'java[^\\w]', 'Python', '[^\\w]SAS[^\\w]','SQL']\n",
    "#^ = Not, \\w alphaneumeric chars\n",
    "#java or javascript\n",
    "# ( space before or after R\n",
    "# ds['R'] = ds['Job_Description'].str.contains(' R[^\\w]', case=False).astype(int)\n",
    "\n",
    "\n",
    "for i in languages_list:\n",
    "    total_count = sum(ds['Job_Description'].str.contains(i, case=False).astype(int))\n",
    "    print(i+ '  ' +  str(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
