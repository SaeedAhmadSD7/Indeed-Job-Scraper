{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INDEED JOB SCRAPPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start off by loading the libraries we will require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can scrap individual links from job posts and use Beautiful Soup to extract the details and job description.\n",
    "But it will be difficult to extract data in such a way because we will be redirected to a unique page each time on the company's website.\n",
    "Instead we will use selenium and click on individual post. After clicking on a job post, javascript runs and we are redirected to indeed's version of job post instead of that on company's page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets define a function which will scrap individual job posts from indeed.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_indeed(search,max_search = 100,save_csv = True):\n",
    "    # search_term is the keyword/designation to be searched\n",
    "    search_term = search.replace(' ','+')                                   \n",
    "    url = 'https://www.indeed.com/jobs?q={}&limit=50&radius=25&start=0'.format(search_term) \n",
    "    \n",
    "    # Start the browser and load the above URL\n",
    "    browser = webdriver.Chrome('C:/Program Files (x86)/chromedriver.exe')\n",
    "    browser.get(url)\n",
    "    \n",
    "    # A popup will appear when we finish loading the page. We cannot proceed without closing it.\n",
    "    # Hence send the command to the browser to close the popup\n",
    "    try:\n",
    "        close_button = browser.find_element_by_id('prime-popover-close-button')\n",
    "        close_button.click()\n",
    "    except (ElementNotVisibleException, NoSuchElementException):\n",
    "        print('No Popup')\n",
    "    \n",
    "    \n",
    "    # Rating is specified as width in the HTML code. According to this width, stars are assigned in the reviews section.\n",
    "    max_rating = browser.find_element_by_class_name('ratings').size['width']\n",
    "    \n",
    "    # Empty dataframe in which we will store our data scraped from job posts\n",
    "    data = pd.DataFrame(columns = ['Job_Title','Company','Estimated_Salary','Date_Posted','Link_Job_Company',\n",
    "                                   'Link_Job_Indeed','Location','Rating_In_Stars','Job_Description'])\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    \n",
    "    # Loop through the pages\n",
    "    for j in range(max_search // 50):\n",
    "        \n",
    "        # All the job posts have class 'row result clickcard' except the last one\n",
    "        # The last job post has class name 'lastRow row result clickcard'\n",
    "        # We will append it to our job_elements list\n",
    "        job_elements =  browser.find_elements_by_xpath(\"//div[@class='row result clickcard']\")\n",
    "        \n",
    "        try:\n",
    "            job_elements.append(browser.find_element_by_xpath(\"//div[@class='lastRow row result clickcard']\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Loop through the individual job posts\n",
    "        for i in range(len(job_elements)):\n",
    "            \n",
    "            \n",
    "            # The link to job post on company's website\n",
    "            redirect_to_comp_site = job_elements[i].find_element_by_tag_name('a').get_attribute('href')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Find estimated salary if mentioned, else assign NA\n",
    "            \n",
    "            try:\n",
    "                estimated_salary = job_elements[i].find_element_by_class_name('no-wrap').text\n",
    "            except NoSuchElementException:\n",
    "                estimated_salary = 'NA'\n",
    "            \n",
    "            \n",
    "            # Get the date when job was posted. Usually in N days ago.  \n",
    "            date = job_elements[i].find_element_by_class_name('date').text \n",
    "            \n",
    "            # Click on the job post\n",
    "            job_elements[i].click()\n",
    "\n",
    "            # Switch to the next browser tab \n",
    "            browser.switch_to_window(browser.window_handles[1])\n",
    "            \n",
    "            # Sleep for minimum 3 seconds because we dont want to create unnecessary load on Indeed's servers\n",
    "            sleep(3 + random.randint(0,3))\n",
    "            \n",
    "            # Link to the Indeed's version of job post\n",
    "            indeed_link = browser.current_url\n",
    "            \n",
    "            # Sometimes Selenium might start scraping before the page finishes loading or \n",
    "            # we might encounter '404 : Job not found error'\n",
    "            # Although these occurences are very rare we don't want our job scrapper to crash.\n",
    "            # Therefore we will retry before moving on.\n",
    "            # If the data was successfully scrapped then it will break out of the while loop\n",
    "            # If we encounter error it will retry again provided the retry count is below 5\n",
    "            \n",
    "            \n",
    "            for i in range(0,5):\n",
    "                try:\n",
    "                    \n",
    "                    title =  browser.find_element_by_class_name('jobtitle').text\n",
    "                    post = browser.find_element_by_id('job_summary').text\n",
    "                    company = browser.find_element_by_class_name('company').text\n",
    "                    location = browser.find_element_by_class_name('location').text\n",
    "                    break\n",
    "                except NoSuchElementException:\n",
    "                    :\n",
    "                    print('Unable to fetch data. Skipping Page.....')\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # We can fetch the job rating by class 'rating' but it rounds up the number stars. \n",
    "            # Hence it will be slightly less accurate\n",
    "            # Instead we will get the CSS attribute of 'rating' and convert it to number of stars ourselves.\n",
    "            try:\n",
    "                rtng_style_str = browser.find_element_by_class_name('rating').get_attribute('style')\n",
    "                rtng_style_int = float(rtng_style_str.split(' ')[1][:-3])\n",
    "                rating = (rtng_style_int*5)/max_rating\n",
    "            except NoSuchElementException:\n",
    "                rating = 'NA'\n",
    "\n",
    "            # For debugging purposes lets log the job post scrapped\n",
    "            print('Completed Post {} of Page {} - {}'.format(i+1,j+1,title))\n",
    "            \n",
    "            # Insert the data into our dataframe\n",
    "            data = data.append({'Job_Title':title,'Company':company,'Link_Job_Company':redirect_to_comp_site,'Estimated_Salary':estimated_salary,'Date_Posted':date,'Link_Job_Indeed':indeed_link,'Location':location,'Rating_In_Stars':rating,'Job_Description':post},ignore_index=True)    \n",
    "            # Close the window and go to the main page\n",
    "            browser.close()\n",
    "            browser.switch_to_window(browser.window_handles[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Change the URL, so as to move on to the next page\n",
    "\n",
    "        y += 50\n",
    "\n",
    "        url = url.replace('start=' + str(x),'start=' +str(x+50))\n",
    "        x+= 50\n",
    "\n",
    "        browser.get(url)\n",
    "        print('Moving on to page ' + str(j+2))\n",
    "        sleep(2)\n",
    "        \n",
    "        # A popover appears when we go to the next page. We will tell the browser to click on close button.\n",
    "        # Although so far for me it has appeared only on 2nd page but I have included the check for every page to be on safer side\n",
    "        try:\n",
    "            \n",
    "            browser.find_element_by_id('popover-x-button').click()\n",
    "        except:\n",
    "            print('No Newsletter Popup Found')\n",
    "            \n",
    "    # Save dataframe to csv     \n",
    "    if save_csv:\n",
    "        data.to_csv(search + '.csv')\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scrapped 1500 job posts for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrap_indeed('machine learning',1500) #Scrapped on 27th Jan\n",
    "scrap_indeed('artificial intelligence',1500) #Scrapped on 27th Jan\n",
    "scrap_indeed('data scientist',1500) #Scrapped on 28th Jan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load all of the saved files and concatenate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv('Data Scientist.csv')\n",
    "ml = pd.read_csv('Machine Learning.csv')\n",
    "ai = pd.read_csv('Artificial Intelligence.csv')\n",
    "ds_main = pd.concat([ds,ml,ai],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since the data belongs to related search terms, they will have duplicate job posts. This will affect our analysis and hence we will remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Lets check how many jobs have same title, company name and job description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2360\n",
       "True     2140\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_rows_bool = ds_main.duplicated(['Job_Title','Company','Job_Description'],keep = 'first')\n",
    "duplicated_rows_bool.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have 2140 duplicate records out of 4500 records. We will remove these records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_main.drop_duplicates(subset=['Job_Title','Company','Job_Description'],inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by removing the stop words from Job Description column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "RE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "# replace '|'-->' ' and drop all stopwords\n",
    "\n",
    "#ds['Job_Description'] = ds['Job_Description'].str.lower()\n",
    "#/ to \" \" cuz or\n",
    "ds_main['Job_Description'] = ds_main['Job_Description'].replace([r'\\|', '/','•',RE_stopwords,'  ','\\n'], [' ',' ','', ' ',' ',' NextLineHere '], regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Frequency\n",
      "Word                     \n",
      "NextLineHere        73977\n",
      "data                12274\n",
      "experience           6225\n",
      "Experience           4750\n",
      "work                 4407\n",
      "business             4230\n",
      "team                 3963\n",
      "The                  3543\n",
      "We                   3330\n",
      "learning             3311\n",
      "machine              3298\n",
      "Data                 3282\n",
      "software             2904\n",
      "years                2794\n",
      "new                  2746\n",
      "development          2742\n",
      "You                  2573\n",
      "skills               2557\n",
      "technical            2429\n",
      "solutions            2245\n",
      "working              2245\n",
      "including            2050\n",
      "-                    2041\n",
      "using                1989\n",
      "related              1907\n",
      "research             1899\n",
      "technology           1830\n",
      "analysis             1813\n",
      "knowledge            1790\n",
      "&                    1717\n",
      "...                   ...\n",
      "looking              1194\n",
      "projects             1192\n",
      "engineering          1191\n",
      "information          1177\n",
      "Machine              1177\n",
      "programming          1176\n",
      "Science              1171\n",
      "advanced             1153\n",
      "understanding        1142\n",
      "make                 1139\n",
      "within               1123\n",
      "Qualifications       1106\n",
      "Learning             1088\n",
      "analytical           1088\n",
      "deep                 1083\n",
      "strong               1075\n",
      "developing           1071\n",
      "learning,            1069\n",
      "well                 1051\n",
      "use                  1044\n",
      "computer             1039\n",
      "customers            1038\n",
      "company              1036\n",
      "services             1023\n",
      "As                   1023\n",
      "Work                 1018\n",
      "field                1008\n",
      "status,              1007\n",
      "provide               999\n",
      "part                  986\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "top_N = 100\n",
    "\n",
    "words = ds_main['Job_Description'].str.cat(sep=' ')\n",
    "words = words.split()\n",
    "\n",
    "result = pd.DataFrame(Counter(words).most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R  1042\n",
      "Python  1461\n",
      "SAS  353\n",
      "SQL  835\n",
      "C\\+\\+  634\n",
      "C[^+]  669\n",
      "Java  760\n",
      "Javascript  169\n",
      "Scala  286\n",
      "C#  169\n",
      "Lua  9\n",
      ".NET  67\n",
      "Hadoop  593\n",
      "MS Excel  16\n",
      "Tableau  246\n",
      "Qlik  19\n",
      "Spark  591\n",
      "ggplot  20\n",
      "dplyr  7\n",
      "tensorflow  270\n",
      "scikit-learn  116\n"
     ]
    }
   ],
   "source": [
    "# Top data science lang (from KDnuggets) and popu langs from (TIOBE inex)\n",
    "\n",
    "languages_list = ['R','Python','SAS','SQL','C\\\\+\\\\+','C[^+]','Java','Javascript','Scala','C#','Lua','.NET']\n",
    "tools_list = ['Hadoop','MS Excel','Tableau','Qlik','Spark']\n",
    "libraries_list = ['ggplot','dplyr','tensorflow','scikit-learn']\n",
    "\n",
    "skills_dict = {'languages':languages_list,'tools':tools_list,'libraries':libraries_list}\n",
    "#^ = Not, \\w alphaneumeric chars\n",
    "#java or javascript\n",
    "# ( space before or after R\n",
    "# ds['R'] = ds['Job_Description'].str.contains(' R[^\\w]', case=False).astype(int)\n",
    "\n",
    "#should not be a part of a word [^\\w]\n",
    "for j in skills_dict.keys():\n",
    "    \n",
    "    for i in range(len(skills_dict[j])):\n",
    "        i_regex = '[^\\w]' + skills_dict[j][i] + '[^\\w]'\n",
    "        ds_main[skills_dict[j][i]] = ds_main['Job_Description'].str.contains(i_regex, case=False).astype(int)\n",
    "        total_count = sum(ds_main[skills_dict[j][i]])\n",
    "        print(skills_dict[j][i] + '  ' +  str(total_count))\n",
    "\n",
    "#Renaming\n",
    "ds_main = ds_main.rename(str,{'C\\+\\+':'C++','C[^+]':'C'}) ",
    "\n# This project is DEPRECATED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
